# Statistical-Tests-using-Python
#1.NORMALITY TEST
Normality tests are used to determine whether a dataset follows a normal distribution, which is a symmetric, bell-shaped probability distribution. Many statistical techniques assume that the data is normally distributed.
Assessing normality is important because it can affect the validity of various statistical methods like t-tests, ANOVA, and regression analysis. If the data is not normally distributed, it may be necessary to use alternative methods or transform the data.
There are several tests for normality, with the most common ones being the Shapiro-Wilk test, Anderson-Darling test, and the Kolmogorov-Smirnov test. These tests compare the data's distribution to a normal distribution and produce a p-value indicating whether the data is likely to be normally distributed or not.
 If the p-value is less than a chosen significance level (e.g., 0.05), you may reject the null hypothesis that the data is normally distributed. If the p-value is greater than the significance level, you fail to reject the null hypothesis, indicating that the data can be considered normally distributed.

 #2.CORRELATION TESTS
 Correlation tests are used to measure the strength and direction of the relationship between two or more variables. They help you understand how changes in one variable relate to changes in another.
 Correlation tests are crucial for exploring relationships in data, identifying patterns, and making predictions. They are commonly used in fields like finance, social sciences, and epidemiology, among others.
 he most common correlation tests include Pearson's correlation coefficient (Pearson's r), Spearman's rank correlation coefficient (Spearman's rho), and Kendall's tau. Pearson's r measures linear relationships, while Spearman's rho and Kendall's tau assess monotonic (non-linear) relationships between variables.
 The correlation coefficient ranges from -1 to 1. A positive value indicates a positive correlation (as one variable increases, the other tends to increase), while a negative value indicates a negative correlation (as one variable increases, the other tends to decrease). A value of 0 means no linear correlation. The magnitude of the coefficient indicates the strength of the relationship, with 1 or -1 indicating a perfect linear relationship.
